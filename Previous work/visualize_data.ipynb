{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Lib\n",
    "import os\n",
    "\n",
    "# Extended Libs #\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Plotting Libs #\n",
    "import folium\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cartopy.feature\n",
    "import cartopy as css\n",
    "\n",
    "# Data processing & matching #\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.interpolate import CubicSpline, interp1d\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, box\n",
    "from roaring_landmask import RoaringLandmask\n",
    "\n",
    "# Choose file #\n",
    "import tkinter as tk\n",
    "\n",
    "# Own functions #\n",
    "from functions import sar_data_processing as sar_dp\n",
    "from functions import norsat_data_processing as norsat_dp\n",
    "from functions import ais_data_processing as ais_dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Libs #\n",
    "import folium\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.feature\n",
    "import cartopy as css\n",
    "from shapely.geometry import Point, box\n",
    "\n",
    "def generate_random_color():\n",
    "        r = lambda: random.randint(0, 255)\n",
    "        return '#{:02x}{:02x}{:02x}'.format(r(), r(), r())\n",
    "def plot_lat_lon_map(dfs_dict : dict , date) -> None:\n",
    "        \"\"\"\n",
    "        Plots latitude and longitude from multiple DataFrames on the same map.\n",
    "\n",
    "        Parameters:\n",
    "        dfs_dict (dict): A dictionary where keys are labels (e.g., date_keys) and values are DataFrames containing 'Latitude' and 'Longitude'.\n",
    "\n",
    "        \"\"\"\n",
    "        # Initialize the map\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "        # Determine the overall extent based on all dataframes\n",
    "        all_lats = []\n",
    "        all_lons = []\n",
    "        for df in dfs_dict.values():\n",
    "            all_lats.extend(df['latitude'])\n",
    "            all_lons.extend(df['longitude'])\n",
    "\n",
    "        # Set the extent (slightly extended) for the map\n",
    "        ax.set_extent([min(all_lons)-5, max(all_lons)+5, min(all_lats)-5, max(all_lats)+5])\n",
    "\n",
    "        # Add features like coastlines and borders\n",
    "        ax.add_feature(cfeature.COASTLINE)\n",
    "        ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "\n",
    "        # Plot each DataFrame's points with different markers or colors\n",
    "        for label, df in dfs_dict.items():\n",
    "            ax.scatter(df['longitude'], df['latitude'], label=label, s=10, transform=ccrs.PlateCarree())\n",
    "\n",
    "        # Add gridlines and labels\n",
    "        gl = ax.gridlines(draw_labels = True, crs = ccrs.PlateCarree(), linestyle='--')\n",
    "        gl.top_labels = False\n",
    "        gl.right_labels = False\n",
    "        gl.xlabel_style = {'size': 12, 'color': 'black'}\n",
    "        gl.ylabel_style = {'size': 12, 'color': 'black'}\n",
    "\n",
    "        # Add a legend to differentiate between different DataFrames\n",
    "        plt.legend(title = \"DataFrames\", loc = 'upper right')\n",
    "\n",
    "        # Add a title\n",
    "        plt.title(f'Mapping of {date}')\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "\n",
    "#\n",
    "def visualize_on_map(matched_data):\n",
    "    # Create a map centered around the average latitude and longitude\n",
    "    map_center = [matched_data['latitude'].mean(), matched_data['longitude'].mean()]\n",
    "    m = folium.Map(location=map_center, zoom_start=6)\n",
    "\n",
    "    # Add SAR detection markers\n",
    "    for idx, row in matched_data.iterrows():\n",
    "        folium.Marker(\n",
    "            location=[row['latitude'], row['longitude']],\n",
    "            popup=f\"SAR Detection: {row['Object_ID']}\",\n",
    "            icon=folium.Icon(color='red', icon='info-sign')\n",
    "        ).add_to(m)\n",
    "\n",
    "    # Add AIS markers\n",
    "    for idx, row in matched_data.iterrows():\n",
    "        folium.Marker(\n",
    "            location=[row['latitude'], row['longitude']],\n",
    "            popup=f\"MMSI number: {row['mmsi']}\",\n",
    "            icon=folium.Icon(color='blue', icon='info-sign')\n",
    "        ).add_to(m)\n",
    "    \n",
    "    # Display the map\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "AIS, SAR, & Norsat. \n",
    "\n",
    "For 3 days\n",
    "- 02-11-2022, AIS & SAR\n",
    "- 03-11-2022, AIS, SAR & Norsat\n",
    "- 05-11-2022, AIS & SAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD data ###\n",
    "\n",
    "## AIS data ##\n",
    "df_ais_0211 = pd.read_csv(filepath_or_buffer =\"C:\\\\Users\\\\abelt\\\\OneDrive\\\\Desktop\\\\Kandidat\\\\ais\\\\ais_110215.csv\")\n",
    "df_ais_0311 = pd.read_csv(filepath_or_buffer =\"C:\\\\Users\\\\abelt\\\\OneDrive\\\\Desktop\\\\Kandidat\\\\ais\\\\ais_110315.csv\")\n",
    "df_ais_0511 = pd.read_csv(filepath_or_buffer =\"C:\\\\Users\\\\abelt\\\\OneDrive\\\\Desktop\\\\Kandidat\\\\ais\\\\ais_1105.csv\")\n",
    "# Compile into a dict #\n",
    "dfs_ais = {'02-11-2022': df_ais_0211, '03-11-2022': df_ais_0311, '05-11-2022' : df_ais_0511}\n",
    "# Format time value dtype into datetime dtype #\n",
    "for df_ais in dfs_ais.values():\n",
    "    df_ais.rename(columns = {'bs_ts':'time', 'lat':'latitude', 'lon':'longitude'}, inplace = True)\n",
    "    df_ais['source'] = 'ais'\n",
    "    df_ais[\"TimeStamp\"] = pd.to_datetime(df_ais['time'])\n",
    "\n",
    "## SAR data ##\n",
    "df_sar_0211 = pd.read_json(path_or_buf = \"C:\\\\Users\\\\abelt\\\\OneDrive\\\\Desktop\\\\Kandidat\\\\sar\\\\Sentinel_1_detection_20221102T1519.json\", orient='index')\n",
    "df_sar_0311 = pd.read_json(path_or_buf = \"C:\\\\Users\\\\abelt\\\\OneDrive\\\\Desktop\\\\Kandidat\\\\sar\\\\Sentinel_1_detection_20221103T154515.json\", orient='index')\n",
    "df_sar_0511 = pd.read_json(path_or_buf = \"C:\\\\Users\\\\abelt\\\\OneDrive\\\\Desktop\\\\Kandidat\\\\sar\\\\Sentinel_1_detection_20221105T162459.json\", orient='index')\n",
    "# Compile into a dict #\n",
    "dfs_sar = {'02-11-2022': df_sar_0211, '03-11-2022': df_sar_0311, '05-11-2022' : df_sar_0511}\n",
    "# Format time value dtype into datetime dtype #\n",
    "for df_sar in dfs_sar.values():\n",
    "    df_sar['source'] = 'sar'\n",
    "    df_sar['Start'] = pd.to_datetime(df_sar['Start'])\n",
    "    df_sar['End'] = pd.to_datetime(df_sar['End'])\n",
    "\n",
    "## Norsat data ##\n",
    "df_norsat_0211 = pd.read_json(path_or_buf = \"C:\\\\Users\\\\abelt\\\\OneDrive\\\\Desktop\\\\Kandidat\\\\norsat\\\\Norsat3-N1-JSON-Message-DK-2022-11-02T151459Z.json\")\n",
    "df_norsat_0311 = pd.read_json(path_or_buf = \"C:\\\\Users\\\\abelt\\\\OneDrive\\\\Desktop\\\\Kandidat\\\\norsat\\\\Norsat3-N1-JSON-Message-DK-2022-11-03T152759Z.json\")\n",
    "df_norsat_0511 = pd.read_json(path_or_buf = \"C:\\\\Users\\\\abelt\\\\OneDrive\\\\Desktop\\\\Kandidat\\\\norsat\\\\Norsat3-N1-JSON-Message-DK-2022-11-05T155259Z.json\")\n",
    "# Compile into a dict #\n",
    "dfs_norsat = {'02-11-2022': df_norsat_0211, '03-11-2022': df_norsat_0311, '05-11-2022' : df_norsat_0511}\n",
    "# Extracting lat, lon for each ship detected #\n",
    "#for df_norsat in dfs_norsat.items():\n",
    " #   df_norsat['source'] = 'norsat'\n",
    "  #  df_norsat = norsat_dp.norsat_formatting(df_norsat)\n",
    "\n",
    "\n",
    "### Display dict keys and column names in dataframes ###\n",
    "print('AIS:\\n', dfs_ais.keys(), '\\n', dfs_ais['02-11-2022'].columns)\n",
    "print('SAR:\\n', dfs_norsat.keys(), '\\n', dfs_sar['02-11-2022'].columns)\n",
    "print('Norsat:\\n', dfs_sar.keys(), '\\n',dfs_norsat['02-11-2022'].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_ais_to_sar(sar_data, ais_data, time_threshold_seconds = 10, dist_threshold = 0.01, size_threshold = 100):\n",
    "    matched_data_pos = []\n",
    "    matched_data = []\n",
    "\n",
    "    for idx, sar_row in sar_data.iterrows():\n",
    "        # Compute the absolute time difference in seconds\n",
    "        time_diff = np.abs(ais_data['TimeStamp'] - sar_row['Start']).astype('timedelta64[s]').astype('int64')\n",
    "        \n",
    "        # Filter AIS data by time proximity\n",
    "        ais_filtered = ais_data[time_diff <= time_threshold_seconds]\n",
    "        \n",
    "        if not ais_filtered.empty:\n",
    "            # Convert coordinates to numpy arrays with float64 dtype\n",
    "            sar_coords = np.array([sar_row['latitude'], sar_row['longitude']], dtype=np.float64).reshape(1, -1)\n",
    "            ais_coords = np.array(ais_filtered[['latitude', 'longitude']], dtype=np.float64)\n",
    "            \n",
    "            # Calculate spatial proximity using cdist\n",
    "            distances = cdist(sar_coords, ais_coords)\n",
    "            \n",
    "            # Find the closest AIS entry within the distance threshold\n",
    "            min_distance_idx = np.argmin(distances)\n",
    "            min_distance = distances[0, min_distance_idx]\n",
    "            \n",
    "            if min_distance < dist_threshold:\n",
    "                closest_ais_row = ais_filtered.iloc[min_distance_idx]\n",
    "                matched_row = {**sar_row.to_dict(), **closest_ais_row.to_dict()}\n",
    "                matched_data_pos.append(matched_row)\n",
    "\n",
    "                # Check if the length and width match closely enough\n",
    "                length_diff = abs(sar_row['width'] - closest_ais_row['width']) / sar_row['width']\n",
    "                width_diff = abs(sar_row['height'] - closest_ais_row['length']) / sar_row['height']\n",
    "                \n",
    "                if length_diff <= size_threshold and width_diff <= size_threshold:\n",
    "                    matched_row = {**sar_row.to_dict(), **closest_ais_row.to_dict()}\n",
    "                    matched_data.append(matched_row)\n",
    "    \n",
    "    processed = [pd.DataFrame(matched_data), pd.DataFrame(matched_data_pos)]\n",
    "    return processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_key = '03-11-2022'\n",
    "delta_time = pd.Timedelta(minutes = 5)\n",
    "\n",
    "# Expanding dfs_sar\n",
    "expanded_sar_df = sar_dp.expand_objects_for_date(dfs_sar = dfs_sar, date_key = date_key)\n",
    "\n",
    "# Filter data between two dates\n",
    "filtered_ais_df = dfs_ais[date_key].loc[(dfs_ais[date_key]['TimeStamp'] >= (dfs_sar[date_key]['Start'][0] - delta_time))\n",
    "                     & (dfs_ais[date_key]['TimeStamp'] <=  (dfs_sar[date_key]['Start'][0] + delta_time))]\n",
    "\n",
    "# Only using class 0 as it is the \"ship\"-like class.\n",
    "filtered_sar_df = expanded_sar_df.loc[expanded_sar_df['class'] == 0]\n",
    "\n",
    "filtered_norsat_df = dfs_norsat[date_key]\n",
    "\n",
    "filter_dfs = {'AIS' : filtered_ais_df, 'SAR' : filtered_sar_df, 'Norsat' : filtered_norsat_df}\n",
    "\n",
    "expanded_df = sar_dp.expand_objects_for_date(dfs_sar, date_key)\n",
    "\n",
    "# Converta values to floats, ints and datetime\n",
    "# Convert to datetime if necessary\n",
    "expanded_sar_df['Start'] = pd.to_datetime(expanded_sar_df['Start'])\n",
    "filter_dfs['AIS'].loc[:, 'TimeStamp'] = pd.to_datetime(filter_dfs['AIS']['TimeStamp'])\n",
    "\n",
    "# Convert latitude and longitude to float if necessary\n",
    "expanded_sar_df.loc[:, 'latitude'] = pd.to_numeric(expanded_sar_df['latitude'], errors='coerce')\n",
    "expanded_sar_df.loc[:, 'longitude'] = pd.to_numeric(expanded_sar_df['longitude'], errors='coerce')\n",
    "filter_dfs['AIS'].loc[:, 'latitude'] = pd.to_numeric(filter_dfs['AIS']['latitude'], errors='coerce')\n",
    "filter_dfs['AIS'].loc[:, 'longitude'] = pd.to_numeric(filter_dfs['AIS']['longitude'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values in latitude or longitude after conversion\n",
    "expanded_sar_df = expanded_sar_df.dropna(subset=['latitude', 'longitude'])\n",
    "filter_dfs['AIS'] = filter_dfs['AIS'].dropna(subset=['latitude', 'longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais_mmsi = dfs_ais[date_key].groupby(by=['mmsi'])\n",
    "#print('Statistics, number of data points for the groups: \\n', ais_mmsi.size().describe())\n",
    "\n",
    "mmsi_numbers = list(ais_mmsi.indices.keys())\n",
    "\n",
    "splines = {}\n",
    "for mmsi in mmsi_numbers:\n",
    "    tes = ais_mmsi.get_group(mmsi)\n",
    "    \n",
    "    # Create time series for interpolation\n",
    "    times = pd.to_numeric(pd.date_range(start=tes['TimeStamp'].min(), end=tes['TimeStamp'].max(), periods=len(tes)*2))\n",
    "    \n",
    "    # Linear interpolation (works even for 2 points)\n",
    "    int_linear = interp1d(pd.to_numeric(tes['TimeStamp']), np.c_[tes['longitude'], tes['latitude']], axis=0, fill_value=\"extrapolate\", kind='linear')\n",
    "\n",
    "    x_linear, y_linear = int_linear(times).T\n",
    "    \n",
    "    # Cubic spline interpolation, only if there are more than 3 points\n",
    "    if len(tes) > 3:\n",
    "        spl_cubic = CubicSpline(tes['TimeStamp'], np.c_[tes['longitude'], tes['latitude']])\n",
    "        x_cubic, y_cubic = spl_cubic(times).T\n",
    "    else:\n",
    "        # If there are less than 4 points, use linear interpolation for both\n",
    "        x_cubic, y_cubic = x_linear, y_linear\n",
    "    \n",
    "    # Store the results in the splines dictionary\n",
    "    splines.update({\n",
    "        mmsi: {\n",
    "            'cubic': {'x': x_cubic, 'y': y_cubic} ,\n",
    "            'linear': {'x': x_linear, 'y': y_linear}\n",
    "        }\n",
    "    })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datafusion_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
