{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Libraries ####\n",
    "## Python Lib ##\n",
    "import os\n",
    "\n",
    "# Extended Libs #\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Plotting Libs #\n",
    "import folium\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data processing & matching #\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.interpolate import CubicSpline, interp1d\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, box\n",
    "from roaring_landmask import RoaringLandmask\n",
    "\n",
    "# Choose file #\n",
    "import tkinter as tk\n",
    "\n",
    "# Own functions #\n",
    "from functions import DataProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLOT OG SPLINE ###\n",
    "def plot_splines_ais_sar_on_map(splines=None, ais_mmsi=None, sar_data=None, norsat_data=None, ais_sar_pol = None):\n",
    "    # Create a base map centered around the mean of all AIS points\n",
    "    map_center = [\n",
    "        sum(value['y'].mean() for _, value in splines.items()) / len(splines),\n",
    "        sum(value['x'].mean() for _, value in splines.items()) / len(splines),\n",
    "    ]\n",
    "\n",
    "    m = folium.Map(location=map_center, zoom_start=6)\n",
    "\n",
    "    # Plot splines\n",
    "    #if splines is not None:\n",
    "     #   for key, value in splines.items():\n",
    "      #      spline_coords = list(zip(value['y'], value['x']))\n",
    "       #     folium.PolyLine(spline_coords, color='blue', weight=2.5, opacity=1).add_to(m)\n",
    "\n",
    "            \n",
    "    # Extract corresponding AIS data\n",
    "    if ais_mmsi is not None and not ais_mmsi.empty:\n",
    "        tes = ais_mmsi.get_group((key,))  # Pass key as a tuple\n",
    "        for _, row in tes.iterrows():\n",
    "                folium.CircleMarker(\n",
    "                    location=[row['latitude'], row['longitude']],\n",
    "                    radius=1.5,\n",
    "                    color='red',\n",
    "                    fill=True,\n",
    "                    fill_opacity=0.7,\n",
    "                    popup=f\"AIS MMSI: {row['mmsi']}\"\n",
    "                ).add_to(m)\n",
    "\n",
    "    if ais_sar_pol is not None: # Add markers for each AIS/SAR point\n",
    "        for mmsi, coords in ais_sar_pol.items():\n",
    "            folium.CircleMarker(\n",
    "\t\t\t\tlocation=[coords['y'], coords['x']],  # Latitude first, then Longitude\n",
    "                radius=1.5,\n",
    "                color='blue',\n",
    "                fill=True,\n",
    "                fill_opacity=0.7,\n",
    "\t\t\t\tpopup=f\"MMSI: {mmsi}\",\n",
    "\t\t\t\t#icon=folium.Icon(color='blue', icon='info-sign')\n",
    "\t\t\t).add_to(m)\n",
    "\t\t\t\n",
    "\n",
    "    # Plot SAR points if the DataFrame is not None and not empty\n",
    "    if sar_data is not None and not sar_data.empty:\n",
    "        for _, row in sar_data.iterrows():\n",
    "            folium.CircleMarker(\n",
    "                location=[row['latitude'], row['longitude']],\n",
    "                radius=2,\n",
    "                color='green',\n",
    "                fill=True,\n",
    "                fill_opacity=0.7,\n",
    "                popup=f\"SAR Object ID: {row['Object_ID']}\"\n",
    "            ).add_to(m)\n",
    "\n",
    "    # Plot Norsat points if the DataFrame is not None and not empty\n",
    "    if norsat_data is not None and not norsat_data.empty:\n",
    "        for idx, row in norsat_data.iterrows():\n",
    "            folium.CircleMarker(\n",
    "                location=[row['latitude'], row['longitude']],\n",
    "                radius=5,\n",
    "                color='black',\n",
    "                fill=True,\n",
    "                fill_opacity=0.7,\n",
    "                popup=f\"idx: {idx}\"\n",
    "            ).add_to(m)\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "base_path = \"C:\\\\Users\\\\abelt\\\\OneDrive\\\\Desktop\\\\Kandidat\\\\\"\n",
    "\n",
    "# File names\n",
    "ais_files = {\n",
    "    '02-11-2022': 'ais\\\\ais_110215.csv',\n",
    "    '03-11-2022': 'ais\\\\ais_110315.csv',\n",
    "    '05-11-2022': 'ais\\\\ais_1105.csv'\n",
    "}\n",
    "\n",
    "sar_files = {\n",
    "    '02-11-2022': 'sar\\\\Sentinel_1_detection_20221102T1519.json',\n",
    "    '03-11-2022': 'sar\\\\Sentinel_1_detection_20221103T154515.json',\n",
    "    '05-11-2022': 'sar\\\\Sentinel_1_detection_20221105T162459.json'\n",
    "}\n",
    "\n",
    "norsat_files = {\n",
    "    '02-11-2022': 'norsat\\\\Norsat3-N1-JSON-Message-DK-2022-11-02T151459Z.json',\n",
    "    '03-11-2022': 'norsat\\\\Norsat3-N1-JSON-Message-DK-2022-11-03T152759Z.json',\n",
    "    '05-11-2022': 'norsat\\\\Norsat3-N1-JSON-Message-DK-2022-11-05T155259Z.json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define date and time filter\n",
    "date_key = '03-11-2022'\n",
    "delta_time = pd.Timedelta(days = 0, hours = 5, minutes = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIS:\n",
      "dict_keys(['02-11-2022', '03-11-2022', '05-11-2022'])\n",
      "Columns: Index(['time', 'callsign', 'cog', 'destination', 'draught', 'eta', 'imo',\n",
      "       'latitude', 'length', 'longitude', 'mmsi', 'nav_status', 'name', 'sog',\n",
      "       'shiptype', 'width', 'source', 'TimeStamp'],\n",
      "      dtype='object')\n",
      "SAR:\n",
      "dict_keys(['02-11-2022', '03-11-2022', '05-11-2022'])\n",
      "Columns: Index(['ProductType', 'Polarization', 'Swath', 'Start', 'End', 'Name',\n",
      "       'Satellite', 'Shape', 'Objects', 'source'],\n",
      "      dtype='object')\n",
      "Norsat:\n",
      "dict_keys(['02-11-2022', '03-11-2022', '05-11-2022'])\n",
      "Columns: Index(['TimeStamp', 'CollectionInformation', 'NRDEmitterPosition',\n",
      "       'NumberCandidates', 'CandidateList', 'source', 'latitude', 'longitude'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Loading and filtering data #\n",
    "# Initialize the DataProcessor class #\n",
    "data_processor = DataProcessor(base_path)\n",
    "\n",
    "# Load AIS, SAR, and Norsat data\n",
    "data_processor.load_ais_data(ais_files)\n",
    "data_processor.load_sar_data(sar_files)\n",
    "data_processor.load_norsat_data(norsat_files)\n",
    "\n",
    "# Expand SAR data for the selected date\n",
    "expanded_sar_df = data_processor.expand_objects_for_date(date_key)\n",
    "# Filter SAR data (only class 0 \"ship\"-like)\n",
    "filtered_sar_df = expanded_sar_df #.loc[expanded_sar_df['class'] == 0]\n",
    "# Applying landmask and appending if ship is on land\n",
    "filtered_sar_df = data_processor.filter_sar_landmask(filtered_sar_df)\n",
    "# Filter AIS data based on SAR timestamps\n",
    "filtered_ais_df = data_processor.filter_ais_data(date_key, delta_time)\n",
    "\n",
    "# Norsat data doesn't need filtering for now\n",
    "filtered_norsat_df = data_processor.dfs_norsat[date_key]\n",
    "\n",
    "# Store filtered data\n",
    "filter_dfs = {'AIS': filtered_ais_df, 'SAR': filtered_sar_df, 'Norsat': filtered_norsat_df}\n",
    "\n",
    "# Convert columns to appropriate data types\n",
    "data_processor.convert_data_types(expanded_sar_df, ['Start'], ['latitude', 'longitude'])\n",
    "data_processor.convert_data_types(filter_dfs['AIS'], ['TimeStamp'], ['latitude', 'longitude'])\n",
    "data_processor.convert_data_types(filter_dfs['SAR'], ['Start'], ['latitude', 'longitude'])\n",
    "\n",
    "\n",
    "# Clean data by removing rows with NaN in latitude and longitude\n",
    "expanded_sar_df = data_processor.clean_data(expanded_sar_df, ['latitude', 'longitude'])\n",
    "filter_dfs['AIS'] = data_processor.clean_data(filter_dfs['AIS'], ['latitude', 'longitude'])\n",
    "\n",
    "# Display data structure\n",
    "data_processor.display_data_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMSI numbers with insufficient data points: 99 out of 2665\n"
     ]
    }
   ],
   "source": [
    "# Grouping mmsi points and interpolations for temporal matching #\n",
    "ais_mmsi = filter_dfs['AIS'].groupby(by=['mmsi'])\n",
    "\n",
    "mmsi_numbers = list(ais_mmsi.indices.keys())\n",
    "\n",
    "splines = {}\n",
    "not_enough = []  # List to store MMSI numbers with insufficient data points\n",
    "\n",
    "for mmsi in mmsi_numbers:\n",
    "    tes = ais_mmsi.get_group((mmsi,))\n",
    "\n",
    "    # Check for NaN values and drop them\n",
    "    if tes['TimeStamp'].isnull().any() or tes['longitude'].isnull().any() or tes['latitude'].isnull().any():\n",
    "        continue  # Skip this group if there are NaN values\n",
    "\n",
    "    # Ensure there are at least two points for interpolation\n",
    "    if len(tes) < 2:\n",
    "        not_enough.append(mmsi)  # Append MMSI to the list\n",
    "        #print(f\"Not enough data points for MMSI {mmsi}. Skipping interpolation.\")\n",
    "        continue\n",
    "\n",
    "    # Create time series for interpolation\n",
    "    times = pd.to_numeric(pd.date_range(start=tes['TimeStamp'].min(), end=tes['TimeStamp'].max(), periods=len(tes)*2))\n",
    "\n",
    "    # Linear interpolation (works even for 2 points)\n",
    "    int_linear = interp1d(pd.to_numeric(tes['TimeStamp']), np.c_[tes['longitude'], tes['latitude']], axis=0, fill_value=\"extrapolate\", kind='linear')\n",
    "\n",
    "    # Cubic spline interpolation, only if there are more than 3 points\n",
    "    if len(tes) > 3:\n",
    "        spl_cubic = CubicSpline(pd.to_numeric(tes['TimeStamp']), np.c_[tes['longitude'], tes['latitude']])\n",
    "\t\t\n",
    "    # Store the results in the splines dictionary\n",
    "    splines[mmsi] = {\n",
    "        'cubic': spl_cubic,\n",
    "        'linear': int_linear\n",
    "    }\n",
    "\n",
    "# After processing, you can check the not_enough list\n",
    "print(f\"MMSI numbers with insufficient data points: {len(not_enough)} out of {len(mmsi_numbers)}\")¨\n",
    "print(f\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_interpolated = {}\n",
    "# Create time series for interpolation\n",
    "for mmsi in mmsi_numbers:\n",
    "    tes = ais_mmsi.get_group((mmsi,))\n",
    "\n",
    "    # Check for NaN values and drop them\n",
    "    if tes['TimeStamp'].isnull().any() or tes['longitude'].isnull().any() or tes['latitude'].isnull().any():\n",
    "        continue  # Skip this group if there are NaN values\n",
    "\n",
    "    times = pd.to_numeric(pd.date_range(start=tes['TimeStamp'].min(), end=tes['TimeStamp'].max(), periods=len(tes)*2))\n",
    "\n",
    "    if len(tes) > 1:\n",
    "        x_linear, y_linear = splines[mmsi]['linear'](times).T\n",
    "    else:\n",
    "        continue\n",
    "\t\n",
    "    x_cubic, y_cubic = splines[mmsi]['cubic'](times).T if len(tes) > 3 else (x_linear, y_linear)\n",
    "\n",
    "    xy_interpolated[mmsi] = {'x' : x_cubic, 'y' : y_cubic}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Landmask filter: ##\n",
      "On water: 6186, on land: 19031 \n",
      "Total number of SAR points: 25217\n"
     ]
    }
   ],
   "source": [
    "## LAND FILTER ##\n",
    "filter_df_sar_0 = filter_dfs['SAR'][filter_dfs['SAR']['on_land'] == 0]\n",
    "filter_df_sar_1 = filter_dfs['SAR'][filter_dfs['SAR']['on_land'] == 1]\n",
    "print(f'## Landmask filter: ##\\nOn water: {len(filter_df_sar_0)}, on land: {len(filter_df_sar_1)} \\nTotal number of SAR points: {len(filter_dfs['SAR'])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais_sar_pol = {}\n",
    "for mmsi in mmsi_numbers:\n",
    "\ttes = ais_mmsi.get_group((mmsi,))\n",
    "\n",
    "\t# Convert 'Start' column to nanoseconds since the epoch (same as pd.to_numeric(pd.date_range(...)))\n",
    "\ttime_convert = pd.to_numeric(pd.to_datetime(tes['time']))\n",
    "\t\t# Now you can compute the mean of the numeric values\n",
    "\ttime_interpol = time_convert.mean()\n",
    "\n",
    "\ttime = pd.to_numeric(time_interpol)\n",
    "\n",
    "\tif len(tes) > 1:\n",
    "\t\t\tx_linear, y_linear = splines[mmsi]['linear'](time)\n",
    "\telse:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\tx_cubic, y_cubic = splines[mmsi]['cubic'](time) if len(tes) > 3 else (x_linear, y_linear)\n",
    "\t\t\n",
    "\tais_sar_pol[mmsi] = {'x' : x_cubic, 'y' : y_cubic}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "#m = plot_splines_ais_sar_on_map(splines = xy_interpolated, ais_mmsi = ais_mmsi, sar_data = filter_df_sar_0, norsat_data = filter_dfs['Norsat'])\n",
    "m = plot_splines_ais_sar_on_map(splines = xy_interpolated, ais_mmsi = None, sar_data = filter_df_sar_0, norsat_data = None, ais_sar_pol = ais_sar_pol)\n",
    "m.save(f'./images/splines_ais_sar_map_{date_key}_on_land_false.html')  # Save to an HTML file if needed\n",
    "\n",
    "# Call the function\n",
    "m = plot_splines_ais_sar_on_map(splines = xy_interpolated, ais_mmsi = None, sar_data = filter_df_sar_1, norsat_data = None, ais_sar_pol = ais_sar_pol)\n",
    "m.save(f'./images/splines_ais_sar_map_{date_key}_on_land_true.html')  # Save to an HTML file if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching:\n",
    "- SAR & AIS\n",
    "- SAR & RF\n",
    "- AIS & RF\n",
    "- SAR, AIS, & RF\n",
    "Create uncertainty chart or correlation value chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datafusion_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
